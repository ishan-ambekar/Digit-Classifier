{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on MNIST dataset.\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits\n",
    "that is commonly used for training various image processing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Flatten \n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"F:/Mnist datasets/train.csv\")\n",
    "test = pd.read_csv(\"F:/Mnist datasets/test.csv\")\n",
    "\n",
    "y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column from X_train\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "#X_train = X_train.values.reshape(-1, 28, 28, 1) \n",
    "test = test.values.reshape(-1,28,28,1)\n",
    "\n",
    "X_train = X_train.astype('float32')/255  #Divide by 255, so that we have pixel values between 0 and 1\n",
    "test = test.astype('float32')/255\n",
    "\n",
    "y_train = to_categorical(y_train, 10)  #The y_train vector has been converted such that it contains the 10 labels representing the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_digit(digit_array, dx, dy, new=0):\n",
    "    return shift(digit_array.reshape(28,28), [dy, dx], cval=0).reshape(784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the original MNIST dataset contains 70000 images. These images are further divided into training and test sets. \n",
    "But this dataset is small. Hence we use data augmentation to slightly modify the images, so that we can increase the size of our\n",
    "training and test sets.\n",
    "\n",
    "Therefore, we have written a shift function which will shift the images by some pixels in horizontal and vertical directons.\n",
    "\n",
    "More data means more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augmented = [X_train]\n",
    "y_train_augmented = [y_train]\n",
    "for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "    shifted_images = np.apply_along_axis(shift_digit, axis=1, arr=X_train, dx=dx, dy=dy)\n",
    "    X_train_augmented.append(shifted_images)\n",
    "    y_train_augmented.append(y_train)\n",
    "\n",
    "X_train_augmented = np.concatenate(X_train_augmented)\n",
    "y_train_augmented = np.concatenate(y_train_augmented)\n",
    "X_train_augmented.shape, y_train_augmented.shape\n",
    "\n",
    "X_train_augmented = X_train_augmented.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three convolution layers\n",
    "\n",
    "model = Sequential() \n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28, 28, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, kernel_size=(3,3),activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 168000 samples, validate on 42000 samples\n",
      "Epoch 1/7\n",
      "168000/168000 [==============================] - 293s 2ms/step - loss: 0.0680 - acc: 0.9787 - val_loss: 0.0395 - val_acc: 0.9874\n",
      "Epoch 2/7\n",
      "168000/168000 [==============================] - 284s 2ms/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0394 - val_acc: 0.9879\n",
      "Epoch 3/7\n",
      "168000/168000 [==============================] - 283s 2ms/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0281 - val_acc: 0.9913\n",
      "Epoch 4/7\n",
      "168000/168000 [==============================] - 283s 2ms/step - loss: 0.0086 - acc: 0.9973 - val_loss: 0.0328 - val_acc: 0.9902\n",
      "Epoch 5/7\n",
      "168000/168000 [==============================] - 283s 2ms/step - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0296 - val_acc: 0.9924\n",
      "Epoch 6/7\n",
      "168000/168000 [==============================] - 283s 2ms/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0392 - val_acc: 0.9910\n",
      "Epoch 7/7\n",
      "168000/168000 [==============================] - 283s 2ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0279 - val_acc: 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1989d10a630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_augmented, y_train_augmented, batch_size=60, epochs=7, verbose=1, validation_split=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# predict results\n",
    "results = model.predict(test)\n",
    "\n",
    "# select the index with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "\n",
    "submission.to_csv(\"F:/cnn_mnist_datagen.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
